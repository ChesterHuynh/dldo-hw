{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(fpath):\n",
    "    data = np.genfromtxt(fpath, dtype=np.int64, skip_header=7)\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = op.abspath(\"\")\n",
    "fpath = op.join(dirname, \"stable6.txt\")\n",
    "\n",
    "X, y = parse_data(fpath)\n",
    "y -= 1  # Index classes starting at 0 for CrossEntropyLoss() to work\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Net(nn.Module): \n",
    "    def __init__(self, n_features, n_classes, hidden_dim1=15, hidden_dim2=15):\n",
    "        super(My_Net, self).__init__()\n",
    "        #feed forward layers\n",
    "        self.layer_1 = nn.Linear(n_features, hidden_dim1)\n",
    "        self.layer_2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.layer_3 = nn.Linear(hidden_dim2, n_classes)\n",
    "\n",
    "        #activations\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        out = self.layer_1(input_data)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer_3(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Hyperparameters to tune\n",
    "hidden_dim1 = 40\n",
    "hidden_dim2 = 30\n",
    "lr = 5e-3\n",
    "\n",
    "if \"stable4\" in fpath:\n",
    "    batch_size = 1\n",
    "elif \"stable5\" in fpath:\n",
    "    batch_size = 32\n",
    "elif \"stable6\" in fpath:\n",
    "    batch_size = 64\n",
    "\n",
    "net = My_Net(n_features, n_classes, hidden_dim1, hidden_dim2)\n",
    "\n",
    "# Standard cross entropy loss for multi-classification tasks\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer. Here we use Adam optimizer.\n",
    "opt = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = torch.Tensor(X_train)\n",
    "Xtest = torch.Tensor(X_test)\n",
    "ytrain = torch.LongTensor(y_train)\n",
    "ytest = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(Xtrain, ytrain)\n",
    "test = torch.utils.data.TensorDataset(Xtest, ytest)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network using Xavier initialization.\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "My_Net(\n",
       "  (layer_1): Linear(in_features=15, out_features=40, bias=True)\n",
       "  (layer_2): Linear(in_features=40, out_features=30, bias=True)\n",
       "  (layer_3): Linear(in_features=30, out_features=6, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval(net, train_loader, verbose=1):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        outputs = net(inputs)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.int() == labels.int()).sum()\n",
    "        loss_sum  += loss(outputs, labels).item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = loss_sum / num_batches\n",
    "    accuracy = correct.item() / total\n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"Train accuracy: {accuracy * 100:.4f} %\")\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test_eval(net, test_loader, verbose=1):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = net(inputs)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.int() == labels.int()).sum()\n",
    "        loss_sum  += loss(outputs, labels).item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = loss_sum / num_batches\n",
    "    accuracy = correct.item() / total\n",
    "        \n",
    "    if verbose:\n",
    "        print(f\"Test accuracy: {accuracy * 100:.4f} %\")\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 1 : \n",
      "Train accuracy: 66.6173 %\n",
      "Test accuracy: 66.6972 %\n",
      "Time lapse: 1.530 secs\n",
      "In epoch 2 : \n",
      "Train accuracy: 66.6173 %\n",
      "Test accuracy: 66.6972 %\n",
      "Time lapse: 1.440 secs\n",
      "In epoch 3 : \n",
      "Train accuracy: 69.5688 %\n",
      "Test accuracy: 69.8708 %\n",
      "Time lapse: 1.370 secs\n",
      "In epoch 4 : \n",
      "Train accuracy: 73.5536 %\n",
      "Test accuracy: 73.2072 %\n",
      "Time lapse: 1.330 secs\n",
      "In epoch 5 : \n",
      "Train accuracy: 74.9967 %\n",
      "Test accuracy: 74.6414 %\n",
      "Time lapse: 1.300 secs\n",
      "In epoch 6 : \n",
      "Train accuracy: 76.1259 %\n",
      "Test accuracy: 75.6688 %\n",
      "Time lapse: 1.310 secs\n",
      "In epoch 7 : \n",
      "Train accuracy: 76.6578 %\n",
      "Test accuracy: 76.2181 %\n",
      "Time lapse: 1.330 secs\n",
      "In epoch 8 : \n",
      "Train accuracy: 76.9761 %\n",
      "Test accuracy: 76.5334 %\n",
      "Time lapse: 1.280 secs\n",
      "In epoch 9 : \n",
      "Train accuracy: 78.0311 %\n",
      "Test accuracy: 77.5099 %\n",
      "Time lapse: 1.310 secs\n",
      "In epoch 10 : \n",
      "Train accuracy: 78.6371 %\n",
      "Test accuracy: 77.9473 %\n",
      "Time lapse: 1.290 secs\n",
      "In epoch 11 : \n",
      "Train accuracy: 79.2867 %\n",
      "Test accuracy: 78.6695 %\n",
      "Time lapse: 1.290 secs\n",
      "In epoch 12 : \n",
      "Train accuracy: 79.2606 %\n",
      "Test accuracy: 78.5576 %\n",
      "Time lapse: 1.300 secs\n",
      "In epoch 13 : \n",
      "Train accuracy: 79.7227 %\n",
      "Test accuracy: 79.0764 %\n",
      "Time lapse: 1.330 secs\n",
      "In epoch 14 : \n",
      "Train accuracy: 80.0323 %\n",
      "Test accuracy: 79.5646 %\n",
      "Time lapse: 1.290 secs\n",
      "In epoch 15 : \n",
      "Train accuracy: 79.9233 %\n",
      "Test accuracy: 79.3205 %\n",
      "Time lapse: 1.300 secs\n",
      "In epoch 16 : \n",
      "Train accuracy: 80.4682 %\n",
      "Test accuracy: 80.0326 %\n",
      "Time lapse: 1.290 secs\n",
      "In epoch 17 : \n",
      "Train accuracy: 80.6296 %\n",
      "Test accuracy: 80.2156 %\n",
      "Time lapse: 1.340 secs\n",
      "In epoch 18 : \n",
      "Train accuracy: 80.8606 %\n",
      "Test accuracy: 80.4699 %\n",
      "Time lapse: 1.340 secs\n",
      "In epoch 19 : \n",
      "Train accuracy: 81.1178 %\n",
      "Test accuracy: 80.8565 %\n",
      "Time lapse: 1.330 secs\n",
      "In epoch 20 : \n",
      "Train accuracy: 81.2050 %\n",
      "Test accuracy: 80.9684 %\n",
      "Time lapse: 1.290 secs\n",
      "In epoch 21 : \n",
      "Train accuracy: 81.3925 %\n",
      "Test accuracy: 80.9887 %\n",
      "Time lapse: 1.310 secs\n",
      "In epoch 22 : \n",
      "Train accuracy: 81.5887 %\n",
      "Test accuracy: 81.4363 %\n",
      "Time lapse: 1.350 secs\n",
      "In epoch 23 : \n",
      "Train accuracy: 81.7413 %\n",
      "Test accuracy: 81.6295 %\n",
      "Time lapse: 1.310 secs\n",
      "In epoch 24 : \n",
      "Train accuracy: 81.7544 %\n",
      "Test accuracy: 81.6601 %\n",
      "Time lapse: 1.290 secs\n",
      "In epoch 25 : \n",
      "Train accuracy: 81.7413 %\n",
      "Test accuracy: 81.6601 %\n",
      "Time lapse: 1.300 secs\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "train_loss_store = []\n",
    "train_acc_store = []\n",
    "test_loss_store = []\n",
    "test_acc_store = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    time1 = time.time()\n",
    "    print(f\"In epoch {epoch+1} : \")\n",
    "    for i, (sample, label) in enumerate(train_loader):\n",
    "        # Set the gradients to zero initially for each batch\n",
    "        opt.zero_grad()\n",
    "        outputs = net(sample)\n",
    "        l = loss(outputs, label)\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    l_temp, acc_temp = train_eval(net, train_loader)\n",
    "    train_loss_store.append(l_temp)\n",
    "    train_acc_store.append(acc_temp)\n",
    "\n",
    "    l_temp, acc_temp = test_eval(net, test_loader)\n",
    "    test_loss_store.append(l_temp)\n",
    "    test_acc_store.append(acc_temp)\n",
    "\n",
    "    time2 = time.time()\n",
    "    print(f\"Time lapse: {round((time2-time1), 2):.3f} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“dldo”",
   "language": "python",
   "name": "dldo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
